{"cells":[{"cell_type":"markdown","source":["### Creating DataFrames"],"metadata":{}},{"cell_type":"code","source":["# spark is an existing SparkSession\ndf = spark.read.csv('/FileStore/tables/people2.csv', inferSchema='true', header='true', nullValue='null')\n\n# Displays the content of the DataFrame to stdout\ndf.show()"],"metadata":{},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":["### Untyped Dataset Operations (aka DataFrame Operations)"],"metadata":{}},{"cell_type":"code","source":["# spark, df are from the previous example\n# Print the schema in a tree format\ndf.printSchema()\n\n# Select only the \"name\" column\ndf.select(\"name\").show()\n\n# Select everybody, but increment the age by 1\ndf.select(df['name'], df['age'] + 1).show()"],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"code","source":["# Select people older than 21\ndf.filter(df['age'] > 21).show()\n\n# Count people by age\ndf.groupBy(\"age\").count().show()"],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":["### Running SQL Queries Programmatically"],"metadata":{}},{"cell_type":"code","source":["# Register the DataFrame as a SQL temporary view\ndf.createOrReplaceTempView(\"people\")\n\nsqlDF = spark.sql(\"SELECT * FROM people\")\nsqlDF.show()"],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":["### Interoperating with RDDs"],"metadata":{}},{"cell_type":"code","source":["from pyspark.sql import Row\n\n# Load a text file and convert each line to a Row.\nlines = sc.textFile(\"/FileStore/tables/people1.txt\")\n\nparts = lines.map(lambda l: l.split(\",\"))\npeople = parts.map(lambda p: Row(name=p[0], age=p[1]))\n\n# Infer the schema, and register the DataFrame as a table.\nschemaPeople = spark.createDataFrame(people)\nschemaPeople.createOrReplaceTempView(\"people_rdd\")\n\n# SQL can be run over DataFrames that have been registered as a table.\nteenagers = spark.sql(\"SELECT name FROM people WHERE age >= 13 AND age <= 19\")\n\n# The results of SQL queries are Dataframe objects.\n# rdd returns the content as an :class:`pyspark.RDD` of :class:`Row`.\nteenNames = teenagers.rdd.map(lambda p: \"Name: \" + p.name).collect()\nfor name in teenNames:\n    print(name)"],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":["### Programmatically Specifying the Schema"],"metadata":{}},{"cell_type":"code","source":["# Import data types\nfrom pyspark.sql.types import *\n\n# Load a text file and convert each line to a Row.\nlines = sc.textFile(\"/FileStore/tables/people2.txt\")\nparts = lines.map(lambda l: l.split(\",\"))\n\n# Each line is converted to a tuple.\npeople = parts.map(lambda p: (p[0], p[1].strip(), p[2], p[3]))\n\n# The schema is encoded in a string.\nschemaString = \"name age city country\"\n\nfields = [StructField(field_name, StringType(), True) for field_name in schemaString.split()]\nschema = StructType(fields)\n\n\n# Apply the schema to the RDD.\nschemaPeople = spark.createDataFrame(people, schema)\n\n# Creates a temporary view using the DataFrame\nschemaPeople.createOrReplaceTempView(\"people\")\n\n# SQL can be run over DataFrames that have been registered as a table.\nresults = spark.sql(\"SELECT name FROM people\")\n\nresults.show()"],"metadata":{},"outputs":[],"execution_count":11},{"cell_type":"code","source":["results.write.csv('/FileStore/tables/output_test')"],"metadata":{},"outputs":[],"execution_count":12}],"metadata":{"name":"BigDataCourse - Spark SQL - Practice","notebookId":4062513539100665},"nbformat":4,"nbformat_minor":0}
