{
    "cells": [

        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Palindroms"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [],
            "source": [
                "# This allows to download web pages\nimport requests\n\n# This is just a way for having a list of lines from a file on the web.\n# Let's remember that spark cannot process files from the web by itself.\n# This is done only as an example, it cannot be done with huge files.\n# Otherwise the machine may have hard times in processing it.\nfile_content = requests.get('https://raw.githubusercontent.com/forons/BigDataExamples/master/files/data.txt').iter_lines()\n\n\ndef reverse(s):\n  rev = ''\n  for i in s: \n    rev = i + rev\n  return rev\n\nwords = sc.parallelize(file_content)\npalindroms = words.filter(lambda x: x == reverse(x))\npalindroms.collect()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Words that occur exactly 5 times"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [],
            "source": [
                "# This allows to download web pages\nimport requests\n# This allows the usage of add, which is a shortcut for 'lambda x, y: x + y'\nfrom operator import add\n\n# This is just a way for having a list of lines from a file on the web.\n# Let's remember that spark cannot process files from the web by itself.\n# This is done only as an example, it cannot be done with huge files.\n# Otherwise the machine may have hard times in processing it.\nfile_content = requests.get('https://raw.githubusercontent.com/forons/BigDataExamples/master/files/inferno.txt').iter_lines()\nlines = sc.parallelize(file_content)\n\n# Here we split on the whitespace each element of the 'file' rdd, which is a line of the file we read, into words\nwords = lines.flatMap(lambda x: x.split(' '))\n\n# We map each word to a tuple with the word itself and a counter initialized to one\nword_pairs = words.map(lambda x: (x, 1))\n\n# We group the elements with the same key (the word in our case) and sum the counters\nword_count = word_pairs.reduceByKey(add)\n\n# x[0] access the key, and x[1] access to the value of the pair/tuple\nword_count.filter(lambda x: x[1] == 5).collect()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Group by occurrences"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [],
            "source": [
                "result = word_count.groupBy(lambda x: x[1]).collect()\n\nseparator = '######'\n# Print a line for each element, separate elements with different occurrences with the separator variable\nfor key, val in result:\n  print(separator)\n  for elem in val:\n    print(elem, key)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Group Anagrams"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "metadata": {},
            "outputs": [],
            "source": [
                "# This function takes a string and outputs a pair, where:\n# - The key is a string with the characters inside the input ordered in alphabetical order\n# - The value is a list with only the input element\n# (we put the string in lower case in order to do not distinguish between 'word' a 'Word')\ndef charsAndSort(x):\n  chars = list(x.lower())\n  chars.sort()\n  return (''.join(chars), [x])\n\ndistinct = lines.flatMap(lambda x: x.split(' ')).distinct()\npairs = distinct.map(charsAndSort)\n# Here it could have been done:\n# from operator import add\n# and instead of 'lambda x, y: x + y' we could have done just 'add'\npartial = pairs.reduceByKey(lambda x, y: x + y)\nanagrams = partial.filter(lambda x: len(x[1]) > 1)\n\nanagrams.collect()"
            ]
        }
    ],
    "metadata": {
        "name": "BigDataCourse - Spark 1 - Exercises",
        "notebookId": 2312768174863409
    },
    "nbformat": 4,
    "nbformat_minor": 0
}
