{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Spark Hands-On"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Parallelize a collection"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [],
            "source": [
                "data = [1, 2, 3, 4, 5]\n\n# sc is the entry point for spark, it is the spark context, and it is already available here.\n# If you run your own notebook or script you need to create it.\ndataRDD = sc.parallelize(data)\n\ndataRDD.collect()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Filtering Operations"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [],
            "source": [
                "def filterOperation(number):\n  return number < 4\n\ndataRDD.filter(filterOperation).collect()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Lambda Functions"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {},
            "outputs": [],
            "source": [
                "dataRDD.filter(lambda x: filterOperation(x)).collect()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "metadata": {},
            "outputs": [],
            "source": [
                "dataRDD.filter(lambda x: x < 4).collect()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Word Count"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "metadata": {},
            "outputs": [],
            "source": [
                "# This allows to download web pages\nimport requests\n# This allows the usage of add, which is a shortcut for 'lambda x, y: x + y'\nfrom operator import add\n\n# This is just a way for having a list of lines from a file on the web.\n# Let's remember that spark cannot process files from the web by itself.\n# This is done only as an example, it cannot be done with huge files.\n# Otherwise the machine may have hard times in processing it.\nfile_content = requests.get('https://raw.githubusercontent.com/forons/BigDataExamples/master/files/inferno.txt').iter_lines()\nfile = sc.parallelize(file_content)\nfile.collect()\n\n# Here we split on the whitespace each element of the 'file' rdd, which is a line of the file we read, into words\nwords = file.flatMap(lambda x: x.split(' '))\n\n# We map each word to a tuple with the word itself and a counter initialized to one\nword_pairs = words.map(lambda x: (x, 1))\n\n# We group the elements with the same key (the word in our case) and sum the counters\nword_count = word_pairs.reduceByKey(add)\n\n# dbfs is the file system of this notebook.\n# If you want to check it, go on the left panel of this notebook and check the \"data\" label\n# You can browse the file system.\n# It creates a folder, were each element of the folder is created by a partition that saves only its data in order to not collect everything into the driver\n# and save time and resources.\nword_count.saveAsTextFile('/dbfs/FileStore/tables/output')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Sort the courters in descending order"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 12,
            "metadata": {},
            "outputs": [],
            "source": [
                "# x[0] access the key, and x[1] access to the value of the pair/tuple\nword_count.sortBy(lambda x: x[1], ascending=False).collect()"
            ]
        }
    ],
    "metadata": {
        "name": "BigDataCourse - Spark 1",
        "notebookId": 2312768174863409
    },
    "nbformat": 4,
    "nbformat_minor": 0
}